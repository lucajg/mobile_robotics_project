{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443b1c0c-dcd0-4cc6-a33e-a9df0d6e9ff0",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project Report\n",
    "### Authors: Victor David S Dubois (344303), Luca Jiménez Glur (345448), Loïc Tronchet (341181), Maria de Lluch Maqueo Colom (374820)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c6dbd-5f75-4823-84c4-705733e9546c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The project consists of controlling a Thymio robot allowing it to follow a path created through global mapping as well as avoiding spontaneous obstacles.  \n",
    "\n",
    "The specific objectives and how they were achieved are described below. \n",
    "\n",
    "*Thymio must avoid obstacles through global navigation, that is, without relying on the sensors.*\n",
    "\n",
    "- The A* algorithm was chosen to create the optimal path for the Thymio to reach its target because it effectively balances between computational efficiency and optimality. Unlike Dijkstra's algorithm, A* explores fewer nodes depending on the value of the function combining the motion and the heuristic cost.\n",
    "\n",
    "*Thymio must go from an arbitrary position to a target, both of which can be moved to see how the system performs.*\n",
    "\n",
    "- The camera gives the position and the orientation of the robot by identifying the magenta and the cyan dots on top of the Thymio robot. These positions will be sent to the A* Algorithm which can effectively create an optimal path. Two different colored dots were chosen because this way they can be easily identified due to their hue and saturation, and the orientation of the robot can be obtained. \n",
    "\n",
    "*Robot position must be estimated and plotted. Create a map where we can see the position of the robot compared to its global plan.*\n",
    "\n",
    "- A window is created where the camera image can be seen. Within this picture, the planned path is illustrated, in addition to the live tracking of the robot by the camera and the kalman filter position estimation. \n",
    "  \n",
    "*Thymio must be able to use local navigation avoiding obstacles placed in its path at any point in time.*\n",
    "\n",
    "- A neural network is used to avoid the local obstacles, changing the velocity of the motors when an object is sensed by one of the seven sensors. Assigning weights allows the motor velocity to be adjusted based on the direction by which the robot is approaching the obstacle. This method was preferred over other local avoidance techniques because it is not computationally expensive and it can be tuned to define how aggressive the behavior is. \n",
    "  \n",
    "*If the robot is kidnapped, that is, placed in a new starting position, the new pose should be estimated.*\n",
    "\n",
    "- Robot kidnapping is detected through the ground sensors and the new optimal path is calculated once the robot is placed on the ground. Through the camera, the new position is obtained. \n",
    "  \n",
    "*If the camera is hidden, the robot should continue its path relying solely on relative sensing.*\n",
    "\n",
    "- The Kalman filter depends on a flag passed by the camera which indicates whether the camera measurements should be used or not. This behavior allows the filter to operate with or without the visual input, that is, only relying on the estimations calculated from the motor velocities.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c93500-a710-447e-bea9-b2fd50eb9b48",
   "metadata": {},
   "source": [
    "### Environment Chosen\n",
    "The map was chosen to be a 9x6 grid with squares of 140 mm x 140 mm, chosen because in this manner the Thymio would fit with a reasonable margin on each side. Considering the width of the Thymio is approximately 11 cm, 140 mm would give it a margin of 15 mm on each side.\n",
    "\n",
    "The obstacles in the map were strategically placed to allow the Thymio to have at least one path where it had to take several turns. Overcrowding with global obstacles was avoided to allow local obstacles to be added and observe the robot’s behavior. The final obstacle distribution was a result of trial and errors that produced the figure below.  \n",
    "\n",
    "The grid representing the map was created using Python code and Chat GPT. The coordinate system was chosen to have its origin (x=0,y=0) in the center of the upper left corner square of the grid. The x axis goes along the width of the grid and increases to the right. The y axis goes along the length of the grid and increases when going down. Due to this choice, it made sense to have angles that increase when going clockwise and an orientation of 0 would correspond to pointing in the increasing x direction (that is, to the right). This description has been summarized in the figure below.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"img\\grid_picture.jpg\" alt=\"Figure 1\" style=\"width: 80%;\"/>\n",
    "    <figcaption>Figure 1</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "\n",
    "In the image, the white cells represent free space while the black cells are obstacles which must be avoided. These squares can be easily detected with the camera due to the high contrast, however, the sensors do not detect them allowing a finite state machine to be implemented whose condition that switches between local and global navigation is triggered by the sensing of an obstacle. \n",
    "\n",
    "The target the Thymio must reach is represented by a cyan colored square to easily identify with the camera and send its coordinates to the A* algorithm. A grid was chosen as the base because in this manner a finite number of cells would be extracted. The obstacles can fit in one of the cells and be aligned with the grid. In the corners of the grid, squares are placed in order to accurately extract the lines and warp the image correctly. Local obstacles are cylindrical pieces that do not occupy the entire cell to allow the Thymio to properly avoid them without hitting them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f572a-9fc0-486b-b2eb-c3c327ac649b",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96eda9f0-d469-4b93-a3cf-71261b5aa37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\victo\\anaconda3\\envs\\bomr\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\victo\\anaconda3\\envs\\bomr\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\victo\\anaconda3\\envs\\bomr\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\victo\\anaconda3\\envs\\bomr\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0c92c1-c2e8-41bb-957d-8bd155788756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:         8317b8c1-1e42-4de6-ae90-56fa4505fd94\n",
      "group id:   72650c50-f991-4c4a-91e2-c87be6abea9b\n",
      "type:       1 (Thymio II wireless)\n",
      "product id: 8 (Thymio II)\n",
      "name:       Thymio 604\n",
      "status:     2 (available)\n",
      "cap:        3\n",
      "firmware:   14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The exclamation point (!) allow to execute a terminal command in the notebook:\n",
    "!python -m tdmclient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b483937-02da-4a78-ac72-d6f5b43f7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node 8317b8c1-1e42-4de6-ae90-56fa4505fd94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19489a92-d2aa-46eb-b011-d1e3a27daaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from heapq import heappush, heappop # used if you want to use heap queue for priority queue (easier to implement) but not compulsory\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c6642-4a41-4697-a4df-3fd1a67b07ca",
   "metadata": {},
   "source": [
    "## 1. Vision Processing\n",
    "The vision system takes the images captured by the camera and locates the robot, obstacles, goal and start position. \n",
    "\n",
    "To achieve this, the vision is divided into __four main tasks__: \n",
    "1. Detect the map and transform the image to a full-frame top-down view of the map.\n",
    "2. Detect the obstacles\n",
    "3. Detect the goal\n",
    "4. Locate the robot\n",
    "### Detailed explanation of each task:\n",
    "1. #### Map Detection and Perspective Transformation  \n",
    "To detect the map, we rely on four distinct squares, one positioned in each corner. In the HSV color space, the color of these squares is significantly different from the other colors in the image. After smoothing the image with a Gaussian filter, by calculating the difference in Hue values between each pixel and the reference color, we generate a mask. In this case, the reference Hue value is so distinct that we can depend solely on the Hue channel.  \n",
    "Using this mask, contours can be easily detected and classified as squares or not based on their area and the polygon they most closely match. Once four squares are identified, the algorithm determines their centroids and edge lengths.  \n",
    "To isolate the map from the rest of the frame, we perform a perspective transformation. This process uses the four computed centroids, relocating the points to align with the map's corners. The OpenCV function *getPerspectiveTransform()* is used to calculate the transformation matrix, which can then be applied to every subsequent frame.  \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/square_frame_contour.png\" alt=\"Figure 1\" style=\"width: 70%;\"/>\n",
    "    <figcaption>Figure 2: Corner squares detection</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/transformed_frame.png\" alt=\"Figure 2\" style=\"width: 70%;\"/>\n",
    "    <figcaption>Figure 3: Frame after perspective transformation</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "\n",
    "2. #### Detecting Global Obstacles  \n",
    "Global obstacles are represented as black tiles on the map grid. To detect them, we examine each tile individually to determine whether it is predominantly black or white.  \n",
    "Since this is a black-and-white classification problem, the frame is first converted to grayscale. A Gaussian blur is then applied to reduce noise, followed by a threshold to enhance contrast. We use a binary threshold, restricting values to 0 or 255, along with Otsu’s binarization algorithm. This ensures the threshold value adapts dynamically across the map, improving performance under varying lighting conditions.  \n",
    "After preprocessing, each tile is masked using its centroid and edge size. The mean value of the masked area is calculated, and if the result is below a specified threshold (60 in our case), the tile is classified as black and thus identified as an obstacle.  \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/obstacles_blur.png\" alt=\"Figure 1\" style=\"width: 80%;\"/>\n",
    "    <figcaption>Figure 4: Frame after Gaussian Filter</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/obstacles_thresh.png\" alt=\"Figure 2\" style=\"width: 80%;\"/>\n",
    "    <figcaption>Figure 5: Frame after Otsu's binarization</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "\n",
    "3. #### Detecting the Goal  \n",
    "The goal detection algorithm is similar to the map corner detection algorithm, with slight differences in preprocessing and color differentiation.  \n",
    "For preprocessing, the image is converted to the HSV color space and filtered using a bilateral filter. This filter effectively reduces noise and grain while preserving edges.  \n",
    "Instead of relying solely on the Hue value for color differentiation, we calculate the Euclidean distance using both Hue and Saturation values. Using only the Hue value can cause interference from black tiles, which may reflect similar Hue values to the goal reference color. However, the Saturation values differ significantly and help distinguish the goal from obstacles.  \n",
    "Finally, the algorithm converts the detected goal position from frame coordinates to grid coordinates, allowing the path planning algorithm to interpret it accurately.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/goal_hsv.png\" alt=\"Figure 1\" style=\"width: 120%;\"/>\n",
    "    <figcaption>Figure 6: Frame after conversion to HSV</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/goal_blur.png\" alt=\"Figure 2\" style=\"width: 120%;\"/>\n",
    "    <figcaption>Figure 7: Frame after bilateral filtering</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/goal_mask.png\" alt=\"Figure 3\" style=\"width: 120%;\"/>\n",
    "    <figcaption>Figure 8: Mask with goal highlighted</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "\n",
    "4. #### Robot Detection  \n",
    "To detect and locate the robot, we use two colored dots on its top, aligned parallel to the wheel axis. In the direction of travel, the left dot is magenta, and the right dot is cyan.  \n",
    "Before detection, the image is preprocessed. For circle detection, the frame is converted to grayscale as required by OpenCV’s *HoughCircles()* function. A median blur filter is applied to smooth the image, as it preserves edges better than Gaussian blur, which tends to reduce edge sharpness and complicate detection.  \n",
    "The *HoughCircles()* function, based on a variant of the classic Hough Transform, is used for circle detection (refer to the code for parameter details). After detecting circles, we sort them by color. For each detected circle, we check its proximity to the reference colors and retain only the closest match for each reference color. We use the difference in Hue value. Finally, we ensure the centers of the two circles are not too far apart. We do not validate the circle radii, as this is already controlled by parameters in the Hough Transform function.  \n",
    "With the centers of the two positioning dots identified, the robot's center is calculated as the midpoint between them. Knowing that the dots align with the wheel axis, we determine which is left and right and use trigonometry to compute the robot's heading angle. The position is then converted into millimeters for interpretation by the Kalman filter.  \n",
    "Additionally, the algorithm outputs a tracking flag indicating whether the robot could be successfully tracked. This flag is used by other modules to determine whether the camera data can be relied upon.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/pos_blur.png\" alt=\"Figure 1\" style=\"width: 80%;\"/>\n",
    "    <figcaption>Figure 9: Frame after Median Filter</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"vision_pictures/pos_frame.png\" alt=\"Figure 2\" style=\"width: 80%;\"/>\n",
    "    <figcaption>Figure 10: Frame with robot position</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "\n",
    "### Additionnal information on BGR Vs HSV\n",
    "\n",
    "At the beginning of the project, we used BGR values to detect and compare colors. However, this approach proved highly dependent on constant and uniform lighting across the map. To address this limitation, we switched to the HSV color space, which separates Hue, Saturation, and Value. This separation ensured a consistent Hue under varying lighting conditions, significantly improving the system's robustness to changes in illumination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11e739b-3570-426d-94d2-3c8e8c9a23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for circles\n",
    "class Circle:\n",
    "    def __init__(self, center, radius, color):\n",
    "        self.center = center  # Center position as a tuple (x, y)\n",
    "        self.radius = radius  # Radius as a float\n",
    "        self.color = color    # Color as a tuple (B, G, R)\n",
    "\n",
    "    # Method to verify if the circle color matches a reference color\n",
    "    def isColorMatching(self, ref_color):\n",
    "        # Color distance\n",
    "        ref_color = np.array(ref_color)\n",
    "        dmax = 15 # max eculidian distance for the color to be considered equivalent\n",
    "        d = distance(self.color[0], ref_color[0]) # only interested in the first HSV channel\n",
    "\n",
    "        return d <= dmax\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Circle(center={self.center}, radius={self.radius}, color={self.color})\"\n",
    "    \n",
    "\n",
    "# Function to compute euclidian distance between two vectors\n",
    "def distance(vec1, vec2):\n",
    "    vec1 = np.array(vec1, dtype=np.float32)  # Ensure values are floats\n",
    "    vec2 = np.array(vec2, dtype=np.float32)\n",
    "    return np.linalg.norm(vec1 - vec2) # euclidian distance using NumPy\n",
    "\n",
    "# Function to compute the median point between two circles\n",
    "def circles_med(circle1, circle2):\n",
    "    x1, y1 = circle1.center\n",
    "    x2, y2 = circle2.center\n",
    "\n",
    "    median_x = (x1 + x2)/2\n",
    "    median_y = (y1 + y2)/2\n",
    "\n",
    "    return (median_x, median_y)\n",
    "\n",
    "# Function to compute the heading angle of the robot based on the two detection circles' centers\n",
    "def robot_angle(cyan_center, mag_center):\n",
    "    mag_center = np.array(mag_center, dtype=np.float32)  # Ensure values are floats\n",
    "    cyan_center = np.array(cyan_center, dtype=np.float32)\n",
    "    x_r, y_r = mag_center\n",
    "    x_g, y_g = cyan_center\n",
    "    \n",
    "    # Calculate the direction vector\n",
    "    dx = x_g - x_r\n",
    "    dy = y_g - y_r\n",
    "    \n",
    "    # Compute the angle in radians\n",
    "    angle_radians = np.arctan2(dy, dx)\n",
    "    \n",
    "    return angle_radians\n",
    "\n",
    "# Funtion to detect the 4 corner squares of the map based on a reference color\n",
    "def detect_squares(frame, ref_color):\n",
    "\n",
    "    # Preprocessing\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # Convert the frame to HSV values for better color separation\n",
    "    blur = cv2.GaussianBlur(frame_hsv, (7, 7), 0) # Remove noise\n",
    "    \n",
    "    # Convert the frame to float for computation\n",
    "    frame_float = blur.astype(np.float32)\n",
    "    \n",
    "    # Compute the Euclidean distance from the reference color\n",
    "    ref_color = np.array(ref_color, dtype=np.float32)\n",
    "    distance = np.abs(frame_float[..., 0] - ref_color[0]) # Only the first channel (Hue) is interesting\n",
    "    \n",
    "    # Threshold the image to isolate interest regions\n",
    "    dmax = 10 # Maximum distance between the reference color and the pixel color\n",
    "    mask = (distance <= dmax).astype(np.uint8) * 255 # Mask acting like a threshold\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #Filter contours to detect squares\n",
    "    squares = []\n",
    "    for contour in contours:\n",
    "        # Approximate the contour as a polygon\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        # Check if the contour is a square\n",
    "        if len(approx) == 4 and cv2.isContourConvex(approx):\n",
    "            area = cv2.contourArea(approx)\n",
    "            if 300 < area < 1500:  # Filter by reasonable size\n",
    "                squares.append(approx)\n",
    "\n",
    "    # Ensure exactly 4 squares are detected\n",
    "    if len(squares) != 4:\n",
    "        print(\"Error: Expected 4 squares, but found\", len(squares))\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Succes: 4 squares found !\")\n",
    "\n",
    "    # Compute squares centroids and edge length\n",
    "    centroids = []\n",
    "    for square in squares:\n",
    "        # Calculate centroid\n",
    "        centroid = np.mean(square.reshape(-1, 2), axis=0)\n",
    "        \n",
    "        # Calculate edge length (assuming square shape and uniform edges)\n",
    "        edge_length = np.linalg.norm(square[0] - square[1])  # Distance between the first two vertices\n",
    "        \n",
    "        # Append centroid and edge length\n",
    "        centroids.append(np.append(centroid, edge_length))\n",
    "    \n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute perspective transformation based on the centroids of the corner squares\n",
    "def compute_transform(frame, centroids):\n",
    "    # Sort centroids\n",
    "    y_sort = centroids[centroids[:, 1].argsort()] # Sort based on the y-axis\n",
    "    tops = y_sort[:2,:] # Two upper points\n",
    "    tops = tops[tops[:,0].argsort()] # Sort based on the x-axis\n",
    "    bottoms = y_sort[2:,:] # Two lower points\n",
    "    bottoms = bottoms[bottoms[:,0].argsort()] # Sort based on the x-axis\n",
    "    top_left = tops[0]\n",
    "    top_right = tops[1]\n",
    "    bottom_left = bottoms[0]\n",
    "    bottoms_right = bottoms[1]\n",
    "\n",
    "    # Shift the points from square centroids to map corners\n",
    "    top_left = top_left[:2] - top_left[2]/2\n",
    "    top_right = [top_right[0] + top_right[2]/2, top_right[1] - top_right[2]/2]\n",
    "    bottom_left = [bottom_left[0] - bottom_left[2]/2, bottom_left[1] + bottom_left[2]/2]\n",
    "    bottoms_right = bottoms_right[:2] + bottoms_right[2]/2\n",
    "\n",
    "    # quadrilateral defining the map before transformation\n",
    "    source_quad = np.array([top_left, top_right, bottom_left, bottoms_right], dtype=np.float32)\n",
    "\n",
    "    # Define the destination rectangle\n",
    "    rect_height, rect_width = frame.shape[:2]\n",
    "    destination_rectangle = np.array([\n",
    "        [0, 0],  # Top-left\n",
    "        [rect_width, 0],  # Top-right\n",
    "        [0, rect_height - 1],  # Bottom-left\n",
    "        [rect_width - 1, rect_height - 1]  # Bottom-right\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Compute the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(source_quad, destination_rectangle)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Change of coordinates from frame referential to map referential\n",
    "def frame2mapCoord(frame_coord):\n",
    "    x, y = np.array(frame_coord)\n",
    "    map_coord = np.array([-(y-427), x])\n",
    "    return map_coord\n",
    "\n",
    "# Change of coordinates from map referential to frame referential\n",
    "def map2frameCoord(map_coord):\n",
    "    x, y = np.array(map_coord)\n",
    "    frame_coord = np.array([y, (-x)+427])\n",
    "    return frame_coord\n",
    "\n",
    "# Compute the centroids of every square of a grid\n",
    "def grid_centroids(grid_width, grid_height, frame_width, frame_height):\n",
    "    # Shape of the grid squares\n",
    "    square_width = frame_width/grid_width\n",
    "    square_height = frame_height/grid_height\n",
    "\n",
    "    # Compute centroids\n",
    "    centroids = np.empty((0, 2), dtype=int)\n",
    "    for row in range(grid_height):\n",
    "        for col in range(grid_width):\n",
    "            x = round((col + 0.5) * square_width)\n",
    "            y = round((row + 0.5) * square_height)\n",
    "            centroids = np.vstack((centroids, [x, y]))\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "# Function detecting global obstacles on the map. Black squares are considered global obstacles\n",
    "def detect_obstacles(frame, centroids, square_width, square_height):\n",
    "    # Preprocessing\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert to grayscale for black/white distinction\n",
    "    blur = cv2.GaussianBlur(gray, (5,5),0) # Remove noise\n",
    "    \n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Detect black squares\n",
    "    obstacles = np.empty((0, 2), dtype=int)\n",
    "    for centroid in centroids:\n",
    "        x, y = centroid\n",
    "        \n",
    "        # Create a blank mask\n",
    "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        # Define the square boundaries\n",
    "        top_left = (int(x+square_width/2), int(y-square_height/2))\n",
    "        bottom_right = (int(x-square_width/2), int(y+square_height/2))\n",
    "        \n",
    "        # Draw the region of interest on the mask\n",
    "        cv2.rectangle(mask, top_left, bottom_right, 255, -1) # Black rectangle\n",
    "\n",
    "        # Compute mean value of the region of interest\n",
    "        mean = cv2.mean(thresh, mask=mask)[0]\n",
    "\n",
    "        # Classify as obstacle or not\n",
    "        if mean < 60:\n",
    "            # Convert from pixel to grid position\n",
    "            grid_x = round(x/square_width + 0.5)\n",
    "            grid_y = round(y/square_height + 0.5)\n",
    "            obstacles = np.vstack((obstacles, [grid_x, grid_y]))\n",
    "\n",
    "    return obstacles\n",
    "\n",
    "# Generate an array representing the map [0] are free squares and [-1] are obstacles.\n",
    "# Converts a list of obstacle into a full-size map\n",
    "def generate_map(grid_width, grid_height, obstacles):\n",
    "\n",
    "    global_map = np.zeros((grid_height, grid_width))\n",
    "    \n",
    "    for obstacle in obstacles:\n",
    "\n",
    "        # Change coordinate to align with the path generating function\n",
    "        x, y = obstacle\n",
    "        x_map = 6 - y\n",
    "        y_map = x - 1\n",
    "        global_map[x_map, y_map] = -1\n",
    "\n",
    "    return np.transpose(global_map) # transpose to output a vertical map. Needed but the path generating function\n",
    "\n",
    "# Function to detect the goal\n",
    "def detect_goal(frame, square_width, square_height, ref_color):\n",
    "\n",
    "    # Preprocessing\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # Convert the frame to HSV values for better color separation\n",
    "    blur = cv2.bilateralFilter(frame_hsv,15,75,150)\n",
    "\n",
    "    # Bilateral Filter parameters\n",
    "    # d=15: large filter region for better smoothing\n",
    "    # sigmaColor=75: Default value to avoid too much color mixing\n",
    "    # sigmaSpace=150: High value to filter noisy regions and preserv sharp edges\n",
    "    \n",
    "    # Convert the frame to float for precise computation\n",
    "    frame_float = blur.astype(np.float32)\n",
    "    # Compute the Euclidean distance from the reference color\n",
    "    ref_color = np.array(ref_color, dtype=np.float32)\n",
    "    distance = np.sqrt(np.sum((frame_float[..., :2] - ref_color[:2]) ** 2, axis=-1))\n",
    "\n",
    "    # Threshold the image to isolate region of interest\n",
    "    dmax = 40\n",
    "    mask = (distance <= dmax).astype(np.uint8) * 255\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    #Filter contours to detect squares\n",
    "    squares = []\n",
    "    for contour in contours:\n",
    "        # Approximate the contour\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        # Check if the contour is a square\n",
    "        if len(approx) == 4 and cv2.isContourConvex(approx):\n",
    "            area = cv2.contourArea(approx)\n",
    "            if 100 < area < 1000:  # Filter by reasonable size\n",
    "                squares.append(approx)\n",
    "\n",
    "    # Ensure exactly 1 squares are detected\n",
    "    if len(squares) != 1:\n",
    "        print(\"Error: Expected 1 squares, but found\", len(squares))\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Succes: 1 squares found !\")\n",
    "\n",
    "    # Compute goal centroid\n",
    "    square = squares[0]\n",
    "    goal_centroid = np.mean(square.reshape(-1, 2), axis=0)\n",
    "\n",
    "    x, y = goal_centroid\n",
    "\n",
    "    grid_x = round(x/square_width + 0.5)\n",
    "    grid_y = round(y/square_height + 0.5)\n",
    "    \n",
    "    x_map = 6 - grid_y\n",
    "    y_map = grid_x - 1\n",
    "\n",
    "    goal_grid = np.array([x_map, y_map], dtype=np.int32)\n",
    "    \n",
    "    return goal_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61205966-35ce-40a0-8b54-c2a6635d0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_init():\n",
    "    \"\"\"\n",
    "    Function initialising the map.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Open Camera\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"Failed to open camera. Exiting\")\n",
    "    print(\"Camera opened successfully.\")\n",
    "\n",
    "    # Dumping first frames to make sure the camera is calibrated\n",
    "    for _ in range(100):\n",
    "        cap.read()\n",
    "    \n",
    "    # Map detection and transformation\n",
    "    for attempt in range(10):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Frame capture failed on attempt {attempt + 1}.\")\n",
    "        else:\n",
    "            # Detect centroids of the corner squares\n",
    "            centroids = detect_squares(frame, MAG_REF_HSV)\n",
    "            if centroids is not None and len(centroids)>0:\n",
    "                print(\"Squares detected. Computing transformation matrix.\")\n",
    "                transformation_matrix = compute_transform(frame, centroids)\n",
    "                return transformation_matrix\n",
    "        print(f\"Couldn't find the map :(, attempt {attempt + 1}\")\n",
    "    # If all attempts fail\n",
    "    sys.exit(\"Couldn't find the map. Exiting\")\n",
    "\n",
    "async def cam_robot_pos(frame):\n",
    "    # Reset per frames variables\n",
    "    circles_detected = []\n",
    "    cyan_circles = []\n",
    "    mag_circles = []\n",
    "    robot_position = None\n",
    "    robot_angle_rad = None\n",
    "    robot_position_map_coord = None\n",
    "    tracking = False\n",
    "\n",
    "    # Preprocessing for Hough transform\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.medianBlur(gray,7)\n",
    "\n",
    "    # Compute circle Hough transform\n",
    "    circles_hough = cv2.HoughCircles(\n",
    "        blur, cv2.HOUGH_GRADIENT, dp=1, minDist=20,\n",
    "        param1=150, param2=15, minRadius=5, maxRadius=15\n",
    "    )\n",
    "\n",
    "    # HoughCircles() parameter choice:\n",
    "    # dp = 1: Keep the same resolution between input image and accumulator\n",
    "    # minDist=20: Distance between centers is approx. 50mm = 25 pxl\n",
    "    # param1=150: Higher threshold of the internal threshold. Value fixed experimentaly with cv2.Canny()\n",
    "    # param2=15: low enough to detect all circles. False detected circles are not a big problem because we check color and distance\n",
    "    # minRadius=5: radii approx. 17mm = 8.5 pxl\n",
    "    # maxRadius=15: see above\n",
    "    \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    if circles_hough is not None:\n",
    "        circles_hough = np.uint16(np.around(circles_hough))       \n",
    "        for i,circle in enumerate(circles_hough[0, :]):\n",
    "            center = (circle[0], circle[1])\n",
    "            radius = circle[2]\n",
    "            # Mask for average color calculation\n",
    "            mask = np.zeros(hsv.shape[:2], dtype=np.uint8)\n",
    "            cv2.circle(mask, center, radius, 255, -1)\n",
    "            avg_color = cv2.mean(hsv, mask=mask)[:3] #  alpha channel not useful\n",
    "            circles_detected.append(Circle(center, radius, avg_color))\n",
    "            \n",
    "        if len(circles_detected) > 1:\n",
    "            # Keep track of cyan and magenta\n",
    "            for circle in circles_detected:\n",
    "                # Check if the circle is cyan\n",
    "                if circle.isColorMatching(CYAN_REF_HSV):\n",
    "                    cyan_circles.append(circle)\n",
    "                # Check if the circle is magenta\n",
    "                elif circle.isColorMatching(MAG_REF_HSV):\n",
    "                    mag_circles.append(circle)\n",
    "\n",
    "            # Find the closest cyan and magenta circles to their references\n",
    "            cyan_circle = min(cyan_circles, key=lambda c: distance(c.color, CYAN_REF_HSV), default=None)\n",
    "            mag_circle = min(mag_circles, key=lambda c: distance(c.color, MAG_REF_HSV), default=None)\n",
    "\n",
    "            # Validate and calculate robot position and orientation\n",
    "            if cyan_circle and mag_circle:\n",
    "                dist_between_circles = distance(cyan_circle.center, mag_circle.center)\n",
    "                if dist_between_circles < MAX_CIRCLE_DISTANCE:\n",
    "                    tracking = True\n",
    "\n",
    "                    # Compute robot position and angle\n",
    "                    robot_position = circles_med(cyan_circle, mag_circle)\n",
    "                    robot_angle_rad = robot_angle(cyan_circle.center, mag_circle.center)\n",
    "\n",
    "                    # Update trajectory\n",
    "                    robot_position = tuple(map(int, robot_position))\n",
    "                    robot_position_map_coord = frame2mapCoord(robot_position)\n",
    "\n",
    "    if tracking:\n",
    "        robot_position_map_mm = robot_position_map_coord*(1260/640) - [70 , 70]\n",
    "        return robot_position_map_mm, robot_angle_rad, tracking\n",
    "    else:\n",
    "        print(\"Can't find the robot :(\")\n",
    "        return [-1,-1], -1, tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4707d-d853-4cd0-b25f-b5c4159fd68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T15:07:33.687404Z",
     "iopub.status.busy": "2024-11-22T15:07:33.687404Z",
     "iopub.status.idle": "2024-11-22T15:07:33.702835Z",
     "shell.execute_reply": "2024-11-22T15:07:33.700048Z",
     "shell.execute_reply.started": "2024-11-22T15:07:33.687404Z"
    }
   },
   "source": [
    "## 2. Global Navigation\n",
    "The A* algorithm was selected to find the optimal path (in the sense of lowest cost) for the robot to travel along. The code for this algorithm was obtained from the solutions from week 5 and modified to adapt to our project specifications. \n",
    "\n",
    "In particular, the choice was made to implement the algorithm on a 9x6 grid, with displacement possible to all eight neighbors of a cell. However, movement to diagonal neighbors is only allowed if none of the neighbor cells which are adjacent to the diagonal neighbor are obstacles. In other words, the robot is not allowed to “cut corners” of an obstacle. If this constraint were not implemented, a part of the robot would pass over the obstacle during diagonal displacement. \n",
    "\n",
    "To take this into account, the diagonal neighbors cells were added to the neighbor list, and then a verification is made to check whether the adjacent neighbors are set as obstacles. In the positive case,  the corresponding diagonal neighbor is removed from the potential neighbor list. \n",
    "\n",
    "The g cost of moving to a diagonal was set to be equal to the one of regular adjacent displacement (1) multiplied by a factor root 2. For the heuristic function f, we chose the euclidean distance. This heuristic is slightly optimistic, because the actual path to the goal will always be larger or equal to the euclidean distance to the goal. So the heuristic does not overestimate the path length, therefore it is appropriate to use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9288e00d-3607-4289-8bad-5585902f875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code from exercises week 5, modified A* algorithm, BoMR course\n",
    "def heuristic(a, b):\n",
    "    # Implement the Euclidean distance heuristic\n",
    "    return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n",
    "\n",
    "def in_grid(grid, pos):\n",
    "    return 0 <= pos[0] < grid.shape[0] and 0 <= pos[1] < grid.shape[1]\n",
    "\n",
    "def is_obstacle(grid, pos):\n",
    "    return not in_grid(grid, pos) or (grid[pos] == -1)\n",
    "\n",
    "# A* algorithm implementation in a Grid\n",
    "async def a_star_search(map_grid, start, goal):\n",
    "    # Initialize the open set as a priority queue and add the start node\n",
    "    open_set = []\n",
    "    heappush(open_set, (heuristic(start, goal), 0, start))  # (f_cost, g_cost, position)\n",
    "\n",
    "    # Initialize the came_from dictionary\n",
    "    came_from = {}\n",
    "    # Initialize g_costs dictionary with default value of infinity and set g_costs[start] = 0\n",
    "    g_costs = {start: 0}\n",
    "    # Initialize the explored set\n",
    "    explored = set()\n",
    "    operation_count = 0\n",
    "\n",
    "    while open_set:\n",
    "        # Pop the node with the lowest f_cost from the open set\n",
    "        current_f_cost, current_g_cost, current_pos = heappop(open_set)\n",
    "\n",
    "        # Add the current node to the explored set\n",
    "        explored.add(current_pos)\n",
    "\n",
    "        # For directly reconstruct path\n",
    "        if current_pos == goal:\n",
    "            break\n",
    "        \n",
    "        up         = (current_pos[0] - 1, current_pos[1]    )  # Up\n",
    "        down       = (current_pos[0] + 1, current_pos[1]    )  # Down\n",
    "        left       = (current_pos[0]    , current_pos[1] - 1)  # Left\n",
    "        right      = (current_pos[0]    , current_pos[1] + 1)  # Right\n",
    "        up_left    = (current_pos[0] - 1, current_pos[1] - 1)  # Up Left\n",
    "        down_right = (current_pos[0] + 1, current_pos[1] + 1)  # Down Right\n",
    "        down_left  = (current_pos[0] + 1, current_pos[1] - 1)  # Down Left\n",
    "        up_right   = (current_pos[0] - 1, current_pos[1] + 1)  # Up Right\n",
    "        # Get the neighbors of the current node (up, down, left, right)\n",
    "        neighbors = [up, down, left, right, up_left, down_right, down_left, up_right]\n",
    "        \n",
    "        # exclude diagonal displacements if there is an adjacent obstacle (no corner cutting)\n",
    "        if is_obstacle(map_grid, up) or is_obstacle(map_grid, left) :\n",
    "                neighbors.remove(up_left)\n",
    "        \n",
    "        if is_obstacle(map_grid, up) or is_obstacle(map_grid, right):\n",
    "                neighbors.remove(up_right)\n",
    "                \n",
    "        if is_obstacle(map_grid, down) or is_obstacle(map_grid, right):\n",
    "                neighbors.remove(down_right)\n",
    "                \n",
    "        if is_obstacle(map_grid, down) or is_obstacle(map_grid, left):\n",
    "                neighbors.remove(down_left)\n",
    "            \n",
    "                \n",
    "        for neighbor in neighbors:\n",
    "            # Check if neighbor is within bounds and not an obstacle\n",
    "            if (in_grid(map_grid,neighbor)) and map_grid[neighbor[0], neighbor[1]] != -1 and neighbor not in explored:\n",
    "                # Calculate tentative_g_cost\n",
    "                tentative_g_cost = current_g_cost + heuristic(current_pos, neighbor)\n",
    "                \n",
    "                # If this path to neighbor is better than any previous one\n",
    "                if neighbor not in g_costs or tentative_g_cost < g_costs[neighbor]:\n",
    "                    # Update came_from, g_costs, and f_cost\n",
    "                    came_from[neighbor] = current_pos\n",
    "                    g_costs[neighbor] = tentative_g_cost\n",
    "                    f_cost = tentative_g_cost + heuristic(neighbor, goal)\n",
    "\n",
    "                    # Add neighbor to open set\n",
    "                    heappush(open_set, (f_cost, tentative_g_cost, neighbor))\n",
    "                    operation_count += 1\n",
    "\n",
    "    # Reconstruct path\n",
    "    if current_pos == goal:\n",
    "        path = []\n",
    "        while current_pos in came_from:\n",
    "            path.append(current_pos)\n",
    "            current_pos = came_from[current_pos]\n",
    "        path.append(start)\n",
    "        return path[::-1], explored, operation_count\n",
    "    else:\n",
    "        # If we reach here, no path was found\n",
    "        return None, explored, operation_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1a92d-6f5b-4cef-99ac-2495d0582ff5",
   "metadata": {},
   "source": [
    "With the most efficient path stored, the critical points, meaning changes in orientation, were stored in a new list that defined the goals for the controller. This list is produced by the following function.\n",
    "\n",
    "**After testing, we figured out that have all the intermediate goals improves tracking of the path. This function is not used further in the code. We kept it in the report for documentation purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a9826f-87d2-4746-86c1-c220942584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the path found# To keep track of how many redundant steps of the path have been removed  by the A* algorithm as an argument and removes the points with a repeated orientation\n",
    "\n",
    "async def goal_list_from_path(path):\n",
    "    goal_list = path.copy()\n",
    "    popped = 0    # To keep track of how many redundant steps of the path have been removed \n",
    "    for i in range(len(path)-2):\n",
    "        # Checks if three consecutive points lie in a straiExght line and removes middle because it is redundant\n",
    "        if (path[i+1][0] - path[i][0] == path[i+2][0] - path[i+1][0]) and (path[i+1][1] - path[i][1] == path[i+2][1] - path[i+1][1]):\n",
    "            goal_list.pop(i+1 - popped)\n",
    "            popped += 1\n",
    "    return goal_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338384bf-b185-42f3-8bd7-f377103b6860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T15:07:47.335947Z",
     "iopub.status.busy": "2024-11-22T15:07:47.335947Z",
     "iopub.status.idle": "2024-11-22T15:07:47.350706Z",
     "shell.execute_reply": "2024-11-22T15:07:47.350706Z",
     "shell.execute_reply.started": "2024-11-22T15:07:47.335947Z"
    }
   },
   "source": [
    "## 3. Local Navigation\n",
    "The local navigation relies on the horizontal sensors of the Thymio. If an object is detected, the robot changes the velocity of the motors to effectively avoid it. The motor velocity changes are made through the weights. Each weight determines how a sensor impacts the robot's movement which is why the weights are larger for the exterior sensors pushing the robot to turn more aggressively than the center weights. The weights were chosen based on the code provided in the third week exercises on Artificial neural networks and adapted according to the behavior desired, through trial and error. The final weights were chosen because they provided a behavior that was not too aggressive, meaning the motor velocities were not too high, yet the obstacle was appropriately avoided.  \n",
    "\n",
    "In addition to the set weights, an additional component was added to ensure the aggressive nature of the local avoidance response was diminished. Specifically, if the robot senses the object getting further away, the weight influence is decreased by a factor of 2. This guarantees that as the robot moves further away from the object, the robot's avoidance response becomes more measured and less aggressive.\n",
    "\n",
    "The code can be seen as a single-layer neural network where the input is the proximity sensor (7 elements) and the output is each motor (2). Parts of the code were taken from the solutions to the third week exercises on Artificial Neural Networks. It is important to note that although the seven sensors were implemented, a circumstance in which having an obstacle behind the Thymio did not arise since the Thymio always moves forward.  \n",
    "\n",
    "**Local Obstacle Positioning conditions**\n",
    "\n",
    "In order for the robot to not collide with global obstacles while in local avoidance state, the local obstacle must be placed in a sufficiently conservative position which does not prompt the robot to fail in a near future. The meaning of sufficient is up to the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a787e241-c0e3-4e8e-8da6-a71a8694fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to obtain the sensor values from the selected type of sensors. Similar to function print_sensor_values obtained from solutions for week 3 exercises\n",
    "\n",
    "async def get_sensor_values(sensor_id, delay=0.01):\n",
    "    await node.wait_for_variables({str(sensor_id)})\n",
    "    await client.sleep(delay)\n",
    "    sensor_values = list(node[sensor_id])  # Retrieve the current values\n",
    "    return sensor_values  # Return the collected sensor values as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad29998b-9995-4e39-abf3-28cb2d457923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to verify if there are any obstacles. Gets the current readings of the get_sensor_values function for the horizontal sensors\n",
    "## If the value of the sensor is above the proximity threshold, it sends a 1\n",
    "\n",
    "async def check_obstacles(proximity_threshold=1500):\n",
    "    horizontal_sensor_values = await get_sensor_values('prox.horizontal')\n",
    "    if any(x > proximity_threshold for x in horizontal_sensor_values):\n",
    "        return 1  # Obstacle detected\n",
    "    return 0  # No obstacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533436d-52c1-4f91-b425-fb6b05afe580",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to avoid obstacles based on neural network\n",
    "## Obtained from solutions for week 3 exercises and adapted to satisfy speed requirements and asynchronous functionality\n",
    "\n",
    "async def obstacle_avoidance(left_output=0, right_output=0, mean_prev=0):\n",
    "    # Define weights for left and right motors\n",
    "    w_l = [ 25,   15, -10, -20, -30,  0, 0]\n",
    "    w_r = [-30, -20, -10,    15, 25, 0,  0]\n",
    "\n",
    "    # Scale factors for sensors and constant factor\n",
    "    sensor_scale = 2000\n",
    "    \n",
    "    # Read proximity sensor values\n",
    "    prox_values = list(node['prox.horizontal'])\n",
    "\n",
    "    mean_values = np.mean(prox_values)\n",
    "\n",
    "    if mean_values < mean_prev:\n",
    "        sensor_scale *= 2\n",
    "    \n",
    "    # Compute motor outputs based on the scaled sensor values\n",
    "    for i in range(len(prox_values)):\n",
    "        # Scale the sensor readings\n",
    "        scaled_value = prox_values[i] // sensor_scale\n",
    "        \n",
    "        # Update outputs for left and right motors\n",
    "        left_output += scaled_value * w_l[i]\n",
    "        right_output += scaled_value * w_r[i]\n",
    "\n",
    "    # Set motor powers\n",
    "    await node.set_variables(motors(int(left_output), int(right_output)))\n",
    "    \n",
    "    return left_output, right_output, mean_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee0ce1-914e-453f-a899-115bce7df937",
   "metadata": {},
   "source": [
    "## 4. Kalman Filter\n",
    "The Kalman Filter’s state representation encapsulates the robot’s position, orientation, linear velocities, and angular velocity, the necessary parameters required for accurate navigation and control in a 2D environment. The state transition model was designed as a linear discrete-time state-space model for each of the components, as in the 8th practice session, assuming constant velocities over constant discrete time intervals (Ts). The time interval was chosen based on the time step delta_t = 0.1s defined in the main control loop and also on the duration of execution of the code, which slightly increases the time between two iterations of the loop. In fact, initially this additional time was not taken into account in the Kalman filter. This led to the estimate of the Kalman filter displaying a non negligible lag behind the true physical position of the robot when the camera was covered and the robot could not be tracked visually. The problem was successfully  identified, so a measure of the duration of execution of the control loop was made with the time library in python. On average, this execution took about 0.02 seconds. So the time constant of the Kalman filter was set to Ts = delta_t + 0.02, that is 0.12 seconds. \n",
    "\n",
    "Two matrices were used to model the uncertainties of the robot motion model, the process noise covariance matrix (Q) and the measurement noise covariance matrix (R). For simplicity, it is assumed that the measurement noise on position and speed are independent.  Similarly, for the noise on the position in x and y, which depends on the angle, the maximum value is taken at all times.\n",
    "\n",
    "*For the Q matrix:*\n",
    "- Position noise (q_x and q_y = 18 mm$^2$): these values were chosen by assuming there are about 3 pixels of error from the camera, which corresponds to  6 mm. Therefore the noise variance is 6^2 = 36 mm$^2$. Assuming half of the noise comes from the process, and half from the measurement, q_p = 36/2 = 18 mm$^2$. To ensure robustness, the worst-case scenario values are consistently used, q_x = 18 mm$^2$ and q_y = 18 mm$^2$.\n",
    "- Orientation noise (q_phi = 0.0025 rad$^2$): It was estimated that the total orientation noise is around 5°, that is, 0.05 rad. Therefore, the orientation variance is 0.0025 rad$^2$. Assuming half comes from the process, and half from the measurement as before, the orientation noise is 0.0025/2 = 0.00125 rad$^2$.\n",
    "- Velocity noise (q_vx and q_vy = 0.74 mm$^2$ /$s^2$): To obtain the velocity noise, the robot was given a constant speed of 119 thymio unit (corresponds to 4cm/s, which is our working speed) on both wheels for 25 seconds. Each 0.1 s, the speed was measured and converted to mm/s using the speed conversion factor. After all the data was collected, the variance was computed resulting in a value of 1.48 mm$^2$/ $s^2$. Taking half of this for the process noise, gives q_vx and q_vy = 0.74 mm$^2$ /$s^2$.\n",
    "- Angular velocity noise (q_vphi = 0.0000779 rad$^2$ /$s^2$) was measured with the same procedure as the velocity noise, however, it was computed based on the measured changes in orientation during the experiment. \n",
    "\n",
    "*For the R matrix:*\n",
    "- Camera measurements for position (r_x and r_y = 18 mm$^2$): Given that is was assumed that half of the noise variance came from the measurement, r_x and r_y are identical to q_x and q_y, that is r_x = 18 mm$^2$ and r_y = 18 mm$^2$.\n",
    "- Camera’s angular measurement noise (r_phi = 0.0025 rad$^2$ ): Following the same reasoning as above, the camera’s angular measurement noise was set to 0.00125 rad$^2$ in the Kalman filter. \n",
    "- Velocity measurement noise (r_vx and r_vy = 0.74 mm$^2$ /$s^2$): Similar as the velocity noise variance, half of the measured value, 1.48 mm$^2$/s, was attributed to the measurement noise. \n",
    "- Angular velocity noise (r_vphi = 0.0000779 rad$^2$ /s$^2$) was derived experimentally, similar to linear velocity noise, by measuring the robot’s rotational motion, then split equally, resulting in a value of 0.4. \n",
    "\n",
    "\n",
    "*Speed conversion factor calculation:* The speed conversion factor was computed by making the Thymio go forward at a set speed (e.g., 50,50 or 100,100) for a set amount of time (e.g., 3 seconds or 6 seconds) and then measuring the physical distance traveled by the robot. The average speed of the robot was then computed by dividing the distance measurement by the time set in the program. This was done for many combinations of speeds and durations, and then the empirical mean of the ratio of average speed to Thymio motor speed was computed. Said ratio corresponds to the speed conversion factor. The factor was found to be approximately 0.33615 mm/s.\n",
    "\n",
    "*Angular speed conversion factor calculation:* In a similar manner, the angular speed conversion factor was found to be 0.003356 rad/s. \n",
    "\n",
    "\n",
    "The Kalman Filter operates in two stages: prediction and correction. In the prediction stage, the robot’s state is estimated based on the motion model, using matrix A and the previous state. This provides an a priori estimate, incorporating process noise from Q. During the correction stage, sensor data is used to refine the predictions.\n",
    "\n",
    "Within the code, two distinct cases exist:\n",
    "1) The camera is operational and it is possible to estimate the different speeds with the motors and the position with the camera.\n",
    "2) The camera is not operational, thus, it is only possible to estimate the different speeds with the motors.\n",
    "\n",
    "To know which case should be employed, the camera sends a flag to indicate whether the camera can see the Thymio or not. As a result, robot movement is possible without the camera, even if the position error increases. This operation requires there is an H matrix and a y vector depending on the case we are in.\n",
    "\n",
    "*Critical angle problem*\n",
    "\n",
    "Since the angles are measured from $-\\pi$ to $\\pi$, there was a problem when the angle estimated by the Kalman filter was close to $pi$, and the one found by the computer vision was close to $-pi$ (or vice-versa). While the physical position of the robot and the estimation were very close, the numerical difference between these two angles was very large, so the kalman filter's estimation would induce a very large correction, generating erratic behavior of the robot. To solve this, some conditions were added to adjust the angle difference in these edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589e1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the remaining constants\n",
    "# units: x [mm],\n",
    "#        y [mm],\n",
    "#        phi [°],\n",
    "#        v_x [mm/s]\n",
    "#        v_y [mm/s]\n",
    "#        phi_point [°/s]\n",
    "#        Ts [s]\n",
    "\n",
    "\n",
    "Ts = 0.12\n",
    "\n",
    "# Matrix that creates model \n",
    "A = np.array([[1, 0, 0, Ts, 0, 0],\n",
    "              [0, 1, 0, 0, Ts, 0],\n",
    "              [0, 0, 1, 0, 0, Ts],\n",
    "              [0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "# Variables for Q matrix to control the variance as a function of the sensors\n",
    "q_x = 18 # mm2\n",
    "q_y = 18 # mm2\n",
    "q_phi = 0.00125 # rad2\n",
    "q_vx = 0.74 # mm2/s2\n",
    "q_vy = 0.74 # mm2/s2\n",
    "q_vphi = 0.000079 # rad2/s2\n",
    "\n",
    "Q = np.array([[q_x, 0, 0, 0, 0, 0],\n",
    "              [0, q_y, 0, 0, 0, 0],\n",
    "              [0, 0, q_phi, 0, 0, 0],\n",
    "              [0, 0, 0, q_vx, 0, 0],\n",
    "              [0, 0, 0, 0, q_vy, 0],\n",
    "              [0, 0, 0, 0, 0, q_vphi]])\n",
    "\n",
    "r_x = 18 # mm2\n",
    "r_y = 18 # mm2\n",
    "r_phi = 0.00125 # rad2\n",
    "r_vx = 0.74 # mm2/s2\n",
    "r_vy = 0.74 # mm2/s2\n",
    "r_vphi = 0.000079 # rad2/s2\n",
    "\n",
    "speed_conv_factor = 0.33615; #valeur moteur -> mm/s\n",
    "angular_speed_conv_factor = 0.1923*np.pi/180 #delta valeur moteur -> rad/s\n",
    "\n",
    "async def kalman_filter(speed_left, speed_right, camera_x, camera_y, camera_phi, camera_flag, x_est_prev, P_est_prev,\n",
    "                  HT=None, HNT=None, RT=None, RNT=None):\n",
    "    \"\"\"\n",
    "    Estimates the current state using input sensor data and the previous state\n",
    "    \n",
    "    param speed_left: measured speed of motor left(Thymio units)\n",
    "    param speed_right: measured speed of motor right(Thymio units)\n",
    "    param camera_x: measured x coordinate with the camera\n",
    "    param camera_y: measured y coordinate with the camera\n",
    "    param camera_phi: measured phi angle with the camera\n",
    "    param camera_flag: flag to know if the camera is operational \n",
    "    param x_est_prev: previous state a posteriori estimation\n",
    "    param P_est_prev: previous state a posteriori covariance\n",
    "    \n",
    "    return x_est: new a posteriori state estimation\n",
    "    return P_est: new a posteriori state covariance\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prediciton through the a priori estimate\n",
    "    # estimated mean of the state\n",
    "    x_est_a_priori = np.dot(A, x_est_prev);\n",
    "    \n",
    "    # Estimated covariance of the state\n",
    "    P_est_a_priori = np.dot(A, np.dot(P_est_prev, A.T));\n",
    "    P_est_a_priori = P_est_a_priori + Q if type(Q) != type(None) else P_est_a_priori\n",
    "\n",
    "    # critical angle condition 1\n",
    "    if (x_est_a_priori[2][0] - camera_phi) > np.pi:\n",
    "        camera_phi += 2 * np.pi\n",
    "\n",
    "    # critical angle condition 2\n",
    "    if (x_est_a_priori[2][0] - camera_phi) < - np.pi:\n",
    "        camera_phi -= 2 * np.pi\n",
    "    \n",
    "    #speed \n",
    "    speed = (speed_left + speed_right)/2\n",
    "    speed_angular = (speed_left - speed_right) #si positif -> sens horaire \n",
    "\n",
    "    phi = x_est_a_priori[2][0]\n",
    "\n",
    "    ## Update         \n",
    "    # y, H, and R for a posteriori estimate, depending on transition\n",
    "    if (camera_flag): \n",
    "        \n",
    "        y = np.array([[camera_x],\n",
    "                      [camera_y],\n",
    "                      [camera_phi],\n",
    "                      [speed*speed_conv_factor*np.cos(phi)],\n",
    "                      [speed *speed_conv_factor*np.sin(phi)],\n",
    "                      [speed_angular*angular_speed_conv_factor]])\n",
    "        H = np.array([[1, 0, 0, 0, 0, 0],\n",
    "                      [0, 1, 0, 0, 0, 0],\n",
    "                      [0, 0, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 0, 0],\n",
    "                      [0, 0, 0, 0, 1, 0],\n",
    "                      [0, 0, 0, 0, 0, 1]])\n",
    "        R = np.array([[r_x, 0, 0, 0, 0, 0],\n",
    "                      [0, r_y, 0, 0, 0, 0],\n",
    "                      [0, 0, r_phi, 0, 0, 0],\n",
    "                      [0, 0, 0, r_vx, 0, 0],\n",
    "                      [0, 0, 0, 0, r_vy, 0],\n",
    "                      [0, 0, 0, 0, 0, r_vphi]])\n",
    "    else:\n",
    "        # no transition, use only the speed\n",
    "        y = np.array([[speed*speed_conv_factor*np.cos(phi)],\n",
    "                      [speed *speed_conv_factor*np.sin(phi)],\n",
    "                      [speed_angular*angular_speed_conv_factor]])\n",
    "        H = np.array([[0, 0, 0, 1, 0, 0],\n",
    "                      [0, 0, 0, 0, 1, 0],\n",
    "                      [0, 0, 0, 0, 0, 1]])\n",
    "        R = np.array([[r_vx, 0, 0],\n",
    "                      [0, r_vy, 0],\n",
    "                      [0, 0, r_vphi]])\n",
    "\n",
    "    # innovation / measurement residual\n",
    "    i = y - np.dot(H, x_est_a_priori);\n",
    "    # measurement prediction covariance\n",
    "    S = np.dot(H, np.dot(P_est_a_priori, H.T)) + R;\n",
    "             \n",
    "    # Kalman gain (tells how much the predictions should be corrected based on the measurements)\n",
    "    K = np.dot(P_est_a_priori, np.dot(H.T, np.linalg.inv(S)));\n",
    "    \n",
    "    \n",
    "    # a posteriori estimate\n",
    "    x_est = x_est_a_priori + np.dot(K,i);\n",
    "    P_est = P_est_a_priori - np.dot(K,np.dot(H, P_est_a_priori));\n",
    "\n",
    "    # make sure the angle stays between pi and -pi\n",
    "    if (x_est[2][0]) > np.pi:\n",
    "        x_est[2][0] -= 2 * np.pi\n",
    "\n",
    "    if (x_est[2][0]) < - np.pi:\n",
    "        x_est[2][0] += 2 * np.pi\n",
    "     \n",
    "    return x_est, P_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1a5e2-9660-4d66-bde6-f4c406bcf60f",
   "metadata": {},
   "source": [
    "## 5. Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb6a4f-3562-4cb9-af94-563a2757b18f",
   "metadata": {},
   "source": [
    "### 5.1 PID Control\n",
    "\n",
    "To effectively create a controller to ensure the robot is able to follow the path, a PID controller based on the orientation error of the robot was employed. A measure of the orientation is obtained through the camera and processed with the Kalman filter to control the robot. A series of functions were used to create the controller based on the error measured between the desired orientation with respect to a certain goal position and the orientation estimate from the Kalman filter. The appropriate gains were selected after testing their effect and identifying the desired response for the robot. The following considerations were taken into account:\n",
    "\n",
    "#### Proportional Gain $K_p$  <!--  (\\(K_p\\))  -->\n",
    "- Controls the strength of the robot's response to the current error.\n",
    "- Determines the time taken to correct the error (avoiding overshooting or overly slow responses).\n",
    "\n",
    "#### Integral Gain  $K_i$ <!--  (\\(K_i\\))  -->\n",
    "- Controls the accumulation of errors in the past through an integration operation.\n",
    "- Ensures there are no steady-state errors by compensating for persistent offsets.\n",
    "\n",
    "#### Derivative Gain  $K_d$ <!--  (\\(K_d\\))  -->\n",
    "- Controls the robot's reaction to the rate of change of the error.\n",
    "- Determines the sensitivity of the controller, ensuring the correction rate is neither too aggressive nor too delayed.\n",
    "\n",
    "To test the controller, a simulator was employed, and the corresponding codes are also included. In the end, we set $K_i$ to 0, which means we only make use of a PD controller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc9f52-c1d4-4826-9695-b677b9bbf986",
   "metadata": {},
   "source": [
    "The following function takes as input the desired x and y positions as well as the current positions estimate (from the kalman filter) to get the errors. This will be used in the PID controller. For position, the Euclidean distance norm is used: \n",
    "\\begin{equation} \n",
    "dist error = \\sqrt{x_{err}^2 + y_{err}^2} \n",
    "\\end{equation}\n",
    "\n",
    "For orientation, the following equation is used:\n",
    "\\begin{equation} \n",
    "\\phi_d = \\arctan2(y_{\\text{err}}, x_{\\text{err}}) \n",
    "\\end{equation} \n",
    "\\begin{equation} \n",
    "\\phi_{\\text{err}} = \\phi_d - \\phi \n",
    "\\end{equation}\n",
    "\n",
    "The function $\\arctan2(y,x)$ returns a value in the interval $(-\\pi,\\pi]$ for the angle $\\theta$ of a vector $(x,y)$, with respect to the $x$ axis.\n",
    "\n",
    "Finally, the orientation error is then normalized to the range -180° to 180° (in case the signs of desired and estimated orientations are different and the sum of their norm is greater than $\\pi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947cf901-9fcb-4217-ac5e-7c7528cda6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function calculates the position and orientation errors based on the estimated and desired positions of the robot\n",
    "# from https://felipenmartins.github.io/Robotics-Simulation-Labs/Lab4/\n",
    "\n",
    "async def get_pose_error(xd, yd, x, y, phi):\n",
    "    \"\"\" Returns the position and orientation errors. \n",
    "        Orientation error is bounded between -pi and +pi radians.\n",
    "        xd: deisred x position\n",
    "        yd: desired y position\n",
    "        x:  estimated x position\n",
    "        y:  estimated y position\n",
    "        phi: estimated orientation\n",
    "    \"\"\"\n",
    "    # Position error:\n",
    "    x_err = xd - x \n",
    "    y_err = yd - y\n",
    "    dist_err = np.sqrt(x_err**2 + y_err**2) # Euclidean distance\n",
    "\n",
    "    # Orientation error\n",
    "    phi_d = np.arctan2(y_err,x_err) # The desired orientation phi_d is computed online from the vector of position error\n",
    "    phi_err = phi_d - phi\n",
    "\n",
    "    # Limits the error to (-pi, pi):\n",
    "    phi_err_correct = np.arctan2(np.sin(phi_err),np.cos(phi_err))\n",
    "\n",
    "    return dist_err, phi_err_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1f17a-a1a1-4964-b8ee-45fc759098d2",
   "metadata": {},
   "source": [
    "This definition defines a PID controller defining the gains for each of the terms. It takes as input the current error, the error from the previous step, the accumulated error and the time step and applies the following equation. The Gains can be declared as a result of trial and error. The PID function is composed of the following:\n",
    "\\begin{equation} P = k_p \\cdot e \\end{equation}\n",
    "\\begin{equation} I = e_{\\text{acc}} + k_i \\cdot e \\cdot \\Delta t \\end{equation}\n",
    "\\begin{equation} D = k_d \\cdot \\frac{e - e_{\\text{prev}}}{\\Delta t} \\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{Output} = k_p \\cdot e + k_i \\cdot \\int e \\, dt + k_d \\cdot \\frac{de}{dt}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af6b96f-1c33-4f30-818b-e67ca3e87739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function implements a \"goal to goal\" behavior meaning that it controls the robot orientation by adjusting the angular speed only. \n",
    "    # The forward speed is supposed to remain constant, equal to a chosen value\n",
    "# from https://felipenmartins.github.io/Robotics-Simulation-Labs/Lab4/\n",
    "\n",
    "async def pid_controller(e, e_prev, e_acc, delta_t, kp=1.0, kd=0, ki=0):\n",
    "    \"\"\" \n",
    "    PID algortithm: must be executed every delta_t seconds\n",
    "    The error e must be calculated as: e = desired_value - actual_value\n",
    "    e_prev contains the error calculated in the previous step.\n",
    "    e_acc contains the integration (accumulation) term.\n",
    "    \"\"\"\n",
    "    P = kp*e                      # Proportional term; kp is the proportional gain\n",
    "    I = e_acc + ki*e*delta_t      # Intergral term; ki is the integral gain\n",
    "    D = kd*(e - e_prev)/delta_t   # Derivative term; kd is the derivative gain\n",
    "\n",
    "    output = P + I + D              # controller output, angular velocity in this case\n",
    "\n",
    "    # store values for the next iteration\n",
    "    e_prev = e     # error value in the previous interation (to calculate the derivative term)\n",
    "    e_acc = I      # accumulated error value (to calculate the integral term)\n",
    "\n",
    "    return output, e_prev, e_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09bac4-a886-4217-b981-0d94473f33c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T14:21:32.089093Z",
     "iopub.status.busy": "2024-11-23T14:21:32.088096Z",
     "iopub.status.idle": "2024-11-23T14:21:32.131148Z",
     "shell.execute_reply": "2024-11-23T14:21:32.124152Z",
     "shell.execute_reply.started": "2024-11-23T14:21:32.089093Z"
    }
   },
   "source": [
    "Taking the output of the controller (angular speed), the speed for each wheel is calculated with the following equations, which relate the forward speed $u_d$ and the angular speed $\\omega_d$ of a differential robot to the angular speeds of each of the wheels (left $\\omega_{l_d}$ and right $\\omega_{r_d}$). The linear speed remains constant to avoid complex behaviors and inaccurate position estimation. $r$ is the radius of the robot’s wheels, $d$ is the base width, that is, the distance between the center of the two wheels.\n",
    "\\begin{equation} \\omega_{r_d} = \\frac{2 \\cdot u_d - d \\cdot \\omega_d}{2 \\cdot r} \\end{equation} \n",
    "\\begin{equation} \\omega_{l_d} = \\frac{2 \\cdot u_d + d \\cdot \\omega_d}{2 \\cdot r} \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5beec4dd-46ae-41ed-b74e-186c9d1932ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function calculates the speeds for the left and right wheels based on the desired values of linear and angular speeds\n",
    "    # The equations come from a model of the kinematics of a differential robot. \n",
    "\n",
    "# modified from https://felipenmartins.github.io/Robotics-Simulation-Labs/Lab4/\n",
    "    # since we chose a different reference frame, we had to invert the wr_d and wl_d from the original code.\n",
    "\n",
    "async def wheel_speed_commands(u_d, w_d, d, r, phi_erreur):\n",
    "    \"\"\"\n",
    "    Convert desired robot speeds to desired wheel abgular speeds\n",
    "    \"\"\"\n",
    "    wr_d = float((2 * u_d - d * w_d) / (2 * r))\n",
    "    wl_d = float((2 * u_d + d * w_d) / (2 * r))\n",
    "        \n",
    "    return wl_d, wr_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbb8cb2-1136-416e-b9e3-360faa08baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_kidnapping():\n",
    "    # Get the current ground sensor values\n",
    "    ground_sensor_values = await get_sensor_values('prox.ground.reflected')\n",
    "\n",
    "    # Threshold to detect lifting\n",
    "    ground_threshold = 25  \n",
    "\n",
    "    # Check if both ground sensors are below the threshold\n",
    "    if ground_sensor_values[1] < ground_threshold and ground_sensor_values[0] < ground_threshold:\n",
    "        print(\"Kidnapping detected! Robot lifted off the ground. Check_kidnapping function\")\n",
    "        return 1  # Return 1 for kidnapping state\n",
    "    else:\n",
    "        return 0  # Normal operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0dd0a-f3ae-419f-b2e3-ad72cab48129",
   "metadata": {},
   "source": [
    "### 5.2 Main Control Loop combining the global and local navigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255bad1-194b-4f6b-b1df-120ac6ab9923",
   "metadata": {},
   "source": [
    "The main control loop is the basis of the robot's navigation system, allowing the robot to follow the global path created with A* while continuously checking for local obstacles and adjusting its movement using the estimates of the kalman filter and the commands of the PD controller. The loop is composed of two while loops to ensure that the code continues to run until the target is reached. The main while loop represents a finite state machine. As seen in the graphic, it is composed of 3 states: global path following (state 0), obstacle avoidance (state 1), and kidnapping (state 2). If no obstacle is detected and the robot is not kidnapped, only state 10 (initial) and state 0 are observed. The state 10 includes the code that needs to be executed each time the robot changes position or is initialized. It was added as part of the exterior while loop to ensure that even if the robot is kidnapped, the code continues to run. When it is set on the ground it recomputes the position and the global path to reenter the finite state machine until the target is reached. \n",
    "\n",
    "If at any point the robot detects a local obstacle, it enters state 1 where the code to follow the global path is overwritten and the robot relies solely on sensor inputs to function. When entering state 1, a single intermediary goal is skipped in the path in order to avoid having the Thymio loop backward when exiting the local obstacle avoidance state. When an obstacle is no longer detected, it continues its path towards the next point in the global navigation trajectory. When returning to the path following state from local obstacle avoidance, to make the transition to the next goal point smoother, the value of the proportional gain is diminished for a defined number of iterations. This diminishes the chances of the robot encountering the same obstacle again. It is important to note that the reduction ratio of the proportional gain is not constant during this period, rather it decreases linearly with time (i.e. each iteration). Its lowest value is 1 after 1.5 seconds, at which point the proportional gain has returned to its original value.\n",
    "\n",
    "Similarly, if the robot no longer detects the ground, the motors are stopped and the finite state machine is exited. The robot returns to state 10, and does not recalculate its new position and trajectory until it is placed on the ground. \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "\n",
    "  <figure style=\"text-align: center;\">\n",
    "    <img src=\"img/main_loop.jpg\" alt=\"Figure 6\" style=\"width: 100%;\"/>\n",
    "    <figcaption>Figure 11</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f6e02a5-c471-4318-937d-2d37180e35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function from exercises week 8, BoMR course\n",
    "\n",
    "def motors(left, right):\n",
    "    return {\n",
    "        \"motor.left.target\": [left],\n",
    "        \"motor.right.target\": [right],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c23035-cbba-46c6-b034-ef72d4cc8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing functionality of the motor (signal reception)\n",
    "node.send_set_variables(motors(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbcab42f-1fa4-4645-ad6c-0b022eed1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing functionality\n",
    "node.send_set_variables(motors(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaba442-32a2-45ff-bcfd-da95454a5603",
   "metadata": {},
   "source": [
    "The main control loop defines several initial values which were established based on testing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "380fd18a-c01d-47a1-aa2d-217473b56af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_control_loop(transformation_matrix):\n",
    "    \"\"\"\n",
    "    Main control loop structured as a finite state machine.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize positions arrays\n",
    "    cam_positions = np.empty((0, 2), dtype=int)\n",
    "    kalman_positions = np.empty((0, 2), dtype=int)\n",
    "    \n",
    "    # Starting state\n",
    "    state = 10\n",
    "    \n",
    "    # Constant forward speed\n",
    "    u_const = 4  # cm/s\n",
    "\n",
    "    # Grid size\n",
    "    grid_width = 9\n",
    "    grid_height = 6\n",
    "\n",
    "    # Get frame for initial obstacle detection and robot position\n",
    "    for i in range(5):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Camera failed for initial obstacle detection and robot position at attempt {i + 1}\")\n",
    "        else:\n",
    "            break\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    frame = cv2.warpPerspective(frame, transformation_matrix, (frame_width, frame_height))\n",
    "    frame = cv2.resize(frame, (640, 427)) # Resizing with 3/2 aspect ratio\n",
    "    frame_height, frame_width = frame.shape[:2] # Update frame shape\n",
    "    \n",
    "    # Map\n",
    "    centroids = grid_centroids(grid_width, grid_height, frame_width, frame_height)\n",
    "    square_width = frame_width/grid_width\n",
    "    square_height = frame_height/grid_height\n",
    "\n",
    "    obstacles = detect_obstacles(frame, centroids, square_width, square_height)\n",
    "    Map = generate_map(grid_width, grid_height, obstacles)\n",
    "\n",
    "    # Robot geometric parameters\n",
    "    base_width   = 9.4   # cm\n",
    "    wheel_radius = 2.2 # cm\n",
    "\n",
    "    # Speed constant (convert thymio motor speed to forward speed)\n",
    "    speed_constant = 0.03375 # cm/s\n",
    "\n",
    "    # Time step\n",
    "    delta_t = 0.1  # seconds\n",
    "\n",
    "    # Initial errors for PID\n",
    "    phi_err_prev = 0\n",
    "    phi_err_acc = 0\n",
    "\n",
    "    # Set initial wheel speed\n",
    "    speed_left = 0\n",
    "    speed_right = 0\n",
    "    \n",
    "    # Signal to finish running the code indefinetely\n",
    "    done = False\n",
    "\n",
    "    # Set initial wheel speed\n",
    "    speed_left = 0\n",
    "    speed_right = 0\n",
    "\n",
    "    # Local obstacle timer\n",
    "\n",
    "    timer = 0 \n",
    "    mean_prev = 0\n",
    "    local_obstacle_flag = 0\n",
    "\n",
    "    P_est = 1000 * np.ones(6)\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if state == 10:\n",
    "            cam_positions = np.empty((0, 2), dtype=int)\n",
    "            kalman_positions = np.empty((0, 2), dtype=int)\n",
    "            \n",
    "            node.send_set_variables(motors(0, 0))\n",
    "            #time.sleep(3) # This can be removed, only used to adjust robot at the beginning and put back from kidnapping\n",
    "\n",
    "            # While loop used to detect whether the robot is on the ground, if it is not, it will be in the loop \n",
    "            threshold = 400\n",
    "            while True:\n",
    "                # Get ground sensor values\n",
    "                ground_sensor_values = await get_sensor_values('prox.ground.reflected')\n",
    "                \n",
    "                if ground_sensor_values[1] >= threshold and ground_sensor_values[0] >= threshold:\n",
    "                    print(\"Robot on the ground :)\")\n",
    "                    break  # if the robot is placed on the ground\n",
    "\n",
    "                # Black frame for kidnapped scenario\n",
    "                kidnapped_frame = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
    "                \n",
    "                # Define the text and font properties\n",
    "                text = \"Kidnapped...\"\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 2\n",
    "                font_thickness = 3\n",
    "                text_color = 255  # White color\n",
    "                \n",
    "                # Get the text size\n",
    "                text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "                text_width, text_height = text_size\n",
    "                \n",
    "                # Calculate the text position to center it\n",
    "                text_x = (frame_width - text_width) // 2\n",
    "                text_y = (frame_height + text_height) // 2  # Add text_height to center vertically\n",
    "                \n",
    "                # Split text into lines and draw each line\n",
    "                cv2.putText(kidnapped_frame, text, (text_x, text_y), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "                # Display the frame\n",
    "                cv2.imshow(win_name, kidnapped_frame)\n",
    "                cv2.waitKey(1)\n",
    "            \n",
    "                print('I am being kidnapped :(, put me on the ground to continue')\n",
    "                time.sleep(delta_t) \n",
    "\n",
    "            time.sleep(3) # So the person can move and the camera can detect the position\n",
    "            \n",
    "            # Goal detection after kidnapping\n",
    "            for attempt in range(10):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Camera failed for obstacle detection and robot position after kidnapping at attempt {attempt + 1}\")\n",
    "                else:\n",
    "                    frame_height, frame_width = frame.shape[:2]\n",
    "                    frame = cv2.warpPerspective(frame, transformation_matrix, (frame_width, frame_height))\n",
    "                    frame = cv2.resize(frame, (640, 427)) # Resizing with 3/2 aspect ratio\n",
    "                    \n",
    "                    SearchGoal = detect_goal(frame, square_width, square_height, CYAN_REF_HSV)\n",
    "                    if SearchGoal is not None:\n",
    "                        print(\"Goal found after kidnapping !\", SearchGoal)\n",
    "                        break\n",
    "                    print(f\"Couldn't find the goal after kidnapping :(, attempt {attempt + 1}\")\n",
    "            SearchGoal = [SearchGoal[1], SearchGoal[0]]\n",
    "\n",
    "            # Robot detection after kidnapping\n",
    "            for attempt in range(10):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Camera failed for obstacle detection and robot position after kidnapping at attempt {attempt + 1}\")\n",
    "                else:\n",
    "                    frame_height, frame_width = frame.shape[:2]\n",
    "                    frame = cv2.warpPerspective(frame, transformation_matrix, (frame_width, frame_height))\n",
    "                    frame = cv2.resize(frame, (640, 427)) # Resizing with 3/2 aspect ratio\n",
    "                    \n",
    "                    SearchStart, StartAngle, tracking = await cam_robot_pos(frame)\n",
    "                    if tracking:\n",
    "                        SearchStart_pxl = map2frameCoord(((SearchStart + [70, 70])*(640/1260)).astype(int)) # Convert to pixels\n",
    "                        cam_positions = np.vstack((cam_positions, SearchStart_pxl))\n",
    "                        print(\"Robot found after kidnapping !\", SearchStart)\n",
    "                        break\n",
    "                print(f\"Couldn't find the robot after kidnapping :(, attempt {attempt + 1}\")\n",
    "            XStart = SearchStart[0]\n",
    "            YStart = SearchStart[1]\n",
    "            SearchStart = np.round(np.array([SearchStart[1]/140, SearchStart[0]/140])).astype(int)\n",
    "            \n",
    "            # Obtaining path\n",
    "            print('Obtaining path')\n",
    "            path, explored, operation_count = await a_star_search(Map, tuple(SearchStart), tuple(SearchGoal))\n",
    "            if path is None:\n",
    "                sys.exit(\"No valid path found\")\n",
    "            # Extract the key_points\n",
    "            goal_list = path\n",
    "\n",
    "            # Kalman initialisation\n",
    "            x_est = np.array([[XStart],\n",
    "                              [YStart],\n",
    "                              [StartAngle],\n",
    "                              [0],\n",
    "                              [0],\n",
    "                              [0]])\n",
    "        \n",
    "            # Start with the first goal\n",
    "            next_goal = 1\n",
    "            x_goal = goal_list[next_goal][1]\n",
    "            y_goal = goal_list[next_goal][0]\n",
    "        \n",
    "            it = 0\n",
    "    \n",
    "            state = 0\n",
    "    \n",
    "        while next_goal < len(goal_list):\n",
    "    \n",
    "            # Check if there are any obstacles\n",
    "            bool_obstacles = await check_obstacles(proximity_threshold=2000)\n",
    "            \n",
    "            # Check to see if robot position changed very suddenly & ground sensors detected lifting\n",
    "            bool_kidnap = await check_kidnapping()\n",
    "\n",
    "            # Combining states\n",
    "            if bool_kidnap == 1:\n",
    "                state = 2  # Kidnapping \n",
    "            elif bool_obstacles == 1:\n",
    "                state = 1  # Obstacle \n",
    "                \n",
    "            else:\n",
    "                state = 0  # Normal operation\n",
    "            \n",
    "            if state == 0:\n",
    "                it += 1\n",
    "\n",
    "                x = x_est[0][0]/140\n",
    "                y = x_est[1][0]/140\n",
    "                phi = x_est[2][0]\n",
    "\n",
    "                # Calculate pose error, only used orientation because the distance error is not necessary\n",
    "                _, phi_err = await get_pose_error(x_goal, y_goal, x, y, phi) # the first returned value is the error in position, but we do not use this information \n",
    "                \n",
    "                # Update orientation PID\n",
    "                k_p = 0.9\n",
    "                if timer > 0:\n",
    "                    timer -= 1\n",
    "                    k_p /= ((timer+4)/4)\n",
    "                    \n",
    "                \n",
    "                phi_control, phi_err_prev, phi_err_acc = await pid_controller(\n",
    "                    phi_err, phi_err_prev, phi_err_acc, delta_t, kp=k_p, kd=0.05, ki=0.0)\n",
    "    \n",
    "                # Map control outputs to desired forward and angular speeds\n",
    "                u_d = u_const  # Constant forward speed\n",
    "                w_d = phi_control  # Angular speed from PID\n",
    "    \n",
    "                # Compute wheel speed commands\n",
    "                left_angular, right_angular = await wheel_speed_commands(u_d, w_d, base_width, wheel_radius, phi_err) # returns angular velocity for each wheel\n",
    "                \n",
    "                # Scale to motor commands\n",
    "                left_speed  = left_angular  * wheel_radius / speed_constant\n",
    "                right_speed = right_angular * wheel_radius / speed_constant\n",
    "    \n",
    "                node.send_set_variables(motors(int(left_speed), int(right_speed)))\n",
    "    \n",
    "                # Check if goal is reached\n",
    "                dist_to_goal = np.sqrt((x_goal - x)**2 + (y_goal - y)**2)*14\n",
    "                if dist_to_goal < 3:  # 3 cm tolerance\n",
    "                    local_obstacle_flag = 0\n",
    "                    print('******************************* goal reached *******************************')\n",
    "                    next_goal += 1\n",
    "                    print(f\"Before loop check, next_goal = {next_goal}, len(goal_list) = {len(goal_list)}\")\n",
    "                    if next_goal < len(goal_list):\n",
    "                        x_goal = goal_list[next_goal][1]\n",
    "                        y_goal = goal_list[next_goal][0]\n",
    "                    elif next_goal == len(goal_list):\n",
    "                        print('Goal Reached!')\n",
    "                        done = True # Flag to terminate outer loop\n",
    "                        break\n",
    "    \n",
    "                # Wait for the next iteration (optional for simulation)\n",
    "            elif state == 1:\n",
    "                if local_obstacle_flag == 0 and next_goal < len(goal_list) - 1:\n",
    "                    next_goal += 1\n",
    "                    x_goal = goal_list[next_goal][1]\n",
    "                    y_goal = goal_list[next_goal][0]\n",
    "                    local_obstacle_flag = 1\n",
    "                timer = 30\n",
    "                \n",
    "                print(\"Obstacle detected - obstacle avoidance\")\n",
    "                left_speed, right_speed, mean_prev = await obstacle_avoidance(left_speed, right_speed, mean_prev)\n",
    "\n",
    "                left_angular = left_speed * speed_constant / wheel_radius\n",
    "                right_angular = right_speed * speed_constant / wheel_radius\n",
    "                    \n",
    "            elif state == 2:\n",
    "                print('Kidnapping')\n",
    "                state = 10\n",
    "                break\n",
    "\n",
    "            speed_left = node[\"motor.left.speed\"] \n",
    "            speed_right = node[\"motor.right.speed\"] \n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                pos_cam, ang_cam, cam_flag = [[-1,-1], -1, False]\n",
    "                print(\"Unable to get frame\")\n",
    "            else:\n",
    "                frame_height, frame_width = frame.shape[:2]\n",
    "                frame = cv2.warpPerspective(frame, transformation_matrix, (frame_width, frame_height))\n",
    "                frame = cv2.resize(frame, (640, 427)) # Resizing with 3/2 aspect ratio\n",
    "                pos_cam, ang_cam, cam_flag = await cam_robot_pos(frame)\n",
    "                if cam_flag:\n",
    "                    pos_cam_pxl = map2frameCoord(((pos_cam + [70, 70])*(640/1260)).astype(int)) # Convert to pixels\n",
    "                    cam_positions = np.vstack((cam_positions, pos_cam_pxl))\n",
    "\n",
    "            x_est, P_est = await kalman_filter(speed_left, speed_right, pos_cam[0], pos_cam[1], ang_cam, cam_flag, x_est, P_est)\n",
    "\n",
    "            x_est_coord = np.array([x_est[0][0], x_est[1][0]])\n",
    "            pos_kalman_pxl = map2frameCoord(((x_est_coord + [70, 70])*(640/1260)).astype(int)) # Convert to pixels\n",
    "            kalman_positions = np.vstack((kalman_positions, pos_kalman_pxl))\n",
    "\n",
    "            # Draw trajectories on frame\n",
    "            # Path\n",
    "            for i in range(1, len(goal_list)):\n",
    "                goal = goal_list[i]\n",
    "                goal = [goal[1], goal[0]]\n",
    "                goal_prev = goal_list[i - 1]\n",
    "                goal_prev = [goal_prev[1], goal_prev[0]]\n",
    "                goal_mm = np.array(goal, dtype=np.float64)*140\n",
    "                goal_pxl = map2frameCoord(((goal_mm + [70, 70])*(640/1260)).astype(int))\n",
    "                goal_mm_prev = np.array(goal_prev, dtype=np.float64)*140\n",
    "                goal_pxl_prev = map2frameCoord(((goal_mm_prev + [70, 70])*(640/1260)).astype(int))\n",
    "                cv2.line(frame, tuple(goal_pxl_prev), tuple(goal_pxl), (0, 0, 255), 2)\n",
    "                \n",
    "            # Camera\n",
    "            for i in range(1, len(cam_positions)):\n",
    "                cv2.line(frame, tuple(cam_positions[i - 1]), tuple(cam_positions[i]), (0, 255, 0), 2)\n",
    "\n",
    "            # Kalman\n",
    "            for i in range(1, len(kalman_positions)):\n",
    "                cv2.line(frame, tuple(kalman_positions[i - 1]), tuple(kalman_positions[i]), (0, 255, 255), 2)\n",
    "\n",
    "            # Highlight robot position\n",
    "            if cam_flag:\n",
    "                cv2.circle(frame, pos_cam_pxl, 5, (255,255,0),-1)\n",
    "\n",
    "\n",
    "            cv2.imshow(win_name, frame)\n",
    "            cv2.waitKey(1)\n",
    "            \n",
    "            time.sleep(delta_t)\n",
    "            \n",
    "    # Stop robot at the end    \n",
    "    left_speed = 0\n",
    "    right_speed = 0\n",
    "    node.send_set_variables(motors(int(left_speed), int(right_speed)))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08bde7-4744-45d2-93e9-3054a610024c",
   "metadata": {},
   "source": [
    "#### Camera window legend\n",
    "- Red line: Global path\n",
    "\n",
    "- Yellow line: Path of the robot estimated by the Kalman filter\n",
    "\n",
    "- Green line: Path of the robot estimated by the camera\n",
    "\n",
    "- Cyan dot: instantaneous position of the robot estimated by the camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13d1a6-2d0d-4cd9-807a-3adeef4659f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera opened successfully.\n",
      "Error: Expected 4 squares, but found 3\n",
      "Couldn't find the map :(, attempt 1\n",
      "Error: Expected 4 squares, but found 3\n",
      "Couldn't find the map :(, attempt 2\n",
      "Succes: 4 squares found !\n",
      "Squares detected. Computing transformation matrix.\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n",
      "I am being kidnapped :(, put me on the ground to continue\n"
     ]
    }
   ],
   "source": [
    "camera_index = 0\n",
    "cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW) # Open camera\n",
    "\n",
    "# Open window for visualization\n",
    "win_name = \"Camera\"\n",
    "cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(win_name, 640, 427)\n",
    "\n",
    "# Initialize variables\n",
    "MAG_REF_HSV = (172, 170, 100)\n",
    "CYAN_REF_HSV = (90, 180, 70)\n",
    "MAX_CIRCLE_DISTANCE = 50\n",
    "\n",
    "transformation_matrix = map_init()\n",
    "\n",
    "await main_control_loop(transformation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d86cc4-02f1-4e0b-b677-97e364a6c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyWindow(win_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69f699-cf06-459c-9ca7-4c92bc0ef713",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Despite successfully accomplishing the goals, there exist several areas of improvement for the project. For instance, the robot can only accurately follow the path at low speeds due to the PID response and the adjustment time. Another improvement lies in the obstacle avoidance logic. If the obstacle leads the robot towards a global obstacle, when it is done avoiding the local obstacle it will possibly go over the global obstacle. Finally, there exists an overshoot originating from the controller. When there are turns, the robot tends to change the angle too much causing very small oscillations to return to the desired direction. These areas of improvement, although important, did not significantly affect the performance of the robot, thus, the original goals set were met. Overall, we consider the code presented a robust implementation that allows the Thymio to follow a calculated optimal path along an obstacle-filled map thanks to a controller, while improving the accuracy of the estimation of its position with a filter and remaining perceptive of local obstacles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b30b88-054e-49b4-b41a-5969ee5992fa",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ca86a-2e66-4712-bdaa-ebd99cade484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T20:18:41.163227Z",
     "iopub.status.busy": "2024-11-24T20:18:41.163227Z",
     "iopub.status.idle": "2024-11-24T20:18:41.178685Z",
     "shell.execute_reply": "2024-11-24T20:18:41.178685Z",
     "shell.execute_reply.started": "2024-11-24T20:18:41.163227Z"
    }
   },
   "source": [
    "PID Explained. (n.d.). PID Controller Explained. Retrieved from https://pidexplained.com/pid-controller-explained/\n",
    "\n",
    "Martins, F. (n.d.). Lab 4: Robotics Simulation Labs. Retrieved November 22, 2024, from https://felipenmartins.github.io/Robotics-Simulation-Labs/Lab4/\n",
    "\n",
    "Mondada, F. (2024). Code for A* Algorithm (Exercise 5). Provided in Mobile Robotics, EPFL. Unpublished class material.\n",
    "\n",
    "Mondada, F. (2024). Code for Artificial Neural Networks and Motor Movement (Exercise 3). Provided in Mobile Robotics, EPFL. Unpublished class material.\n",
    "\n",
    "Wikipedia contributors. (n.d.). HSL and HSV. Wikipedia. Retrieved December 5, 2024, from https://en.wikipedia.org/wiki/HSL_and_HSV\n",
    "\n",
    "OpenCV. (n.d.). Image filtering in OpenCV. Retrieved December 5, 2024, from https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html\n",
    "\n",
    "OpenCV. (n.d.). Contours in OpenCV. Retrieved December 5, 2024, from https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "\n",
    "OpenCV. (n.d.). Hough Circle Transform in OpenCV. Retrieved December 5, 2024, from https://docs.opencv.org/3.4/d4/d70/tutorial_hough_circle.html\n",
    "\n",
    "Stack Overflow. (n.d.). cv2.approxPolyDP, cv2.arcLength – How do these work? Retrieved December 5, 2024, from https://stackoverflow.com/questions/62274412/cv2-approxpolydp-cv2-arclength-how-these-works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
